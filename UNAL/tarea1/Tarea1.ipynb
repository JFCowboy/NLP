{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tarea1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats.stats import pearsonr\n",
    "from scipy.spatial.distance import *\n",
    "from nltk.stem.porter import *\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import sklearn as skl\n",
    "import lexsim #Archivo en el path\n",
    "import numpy as np\n",
    "from struct import *\n",
    "\n",
    "\n",
    "from nltk.corpus import wordnet_ic\n",
    "brown_ic = wordnet_ic.ic('ic-brown.dat')\n",
    "semcor_ic = wordnet_ic.ic('ic-semcor.dat')\n",
    "\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Funciones extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean(texto):\n",
    "    signos_puntuacion=\".,;:'()-[]{}#$&/?!\"\n",
    "    for signo_puntuacion in signos_puntuacion:\n",
    "        texto=texto.replace(signo_puntuacion,\" \")\n",
    "    return texto.split()\n",
    "\n",
    "def leer_dataset_textsim(nombre_archivo):\n",
    "    path=\"./data_textsim/\"\n",
    "    dataset=[]\n",
    "    archivo=open(path+nombre_archivo,\"r\")\n",
    "    for linea in archivo.readlines():\n",
    "        pos_tab=linea.find(\"\\t\")\n",
    "        texto1=clean(linea[:pos_tab])\n",
    "        texto2=clean(linea[pos_tab+1:-1])\n",
    "        dataset.append([texto1,texto2])\n",
    "    archivo.close()\n",
    "    archivo=open(path+nombre_archivo.replace(\".input\",\".gs\"),\"r\")\n",
    "    tmp_gs=[]\n",
    "    for linea in archivo.readlines():\n",
    "        if linea.strip()!=\"\":\n",
    "            gs=float(linea)\n",
    "            tmp_gs.append(gs)\n",
    "    return (dataset,tmp_gs)\n",
    "\n",
    "\n",
    "datasets=[\n",
    "\"2012.input.MSRpar.txt\",\n",
    "\"2012.input.MSRvid.txt\",\n",
    "\"2012.input.OnWN.txt\",\n",
    "\"2012.input.SMTeuroparl.txt\",\n",
    "\"2012.input.SMTnews.txt\",\n",
    "\"2013.input.FNWN.txt\",\n",
    "\"2013.input.headlines.txt\",\n",
    "\"2013.input.OnWN.txt\",\n",
    "\"2013.input.SMT.txt\",\n",
    "\"2014.input.deft-forum.txt\",\n",
    "\"2014.input.deft-news.txt\",\n",
    "\"2014.input.headlines.txt\",\n",
    "\"2014.input.images.txt\",\n",
    "\"2014.input.OnWN.txt\",\n",
    "\"2014.input.tweet-news.txt\",\n",
    "\"2015.input.answers-forums.txt\",\n",
    "\"2015.input.answers-students.txt\",\n",
    "\"2015.input.belief.txt\",\n",
    "\"2015.input.headlines.txt\",\n",
    "\"2015.input.images.txt\",\n",
    "\"2016.input.answer-answer.txt\",\n",
    "\"2016.input.headlines.txt\",\n",
    "\"2016.input.plagiarism.txt\",\n",
    "\"2016.input.postediting.txt\",\n",
    "\"2016.input.question-question.txt\",\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funciones de similitud Textual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#####################################################################3\n",
    "#FUNCIONES DE SIMILITUD TEXTUAL\n",
    "#####################################################################3\n",
    "def STS_monge_elkan(texto1,texto2,lexsim):\n",
    "    suma=0.0\n",
    "    for palabra1 in texto1:\n",
    "        maxsim=0\n",
    "        for palabra2 in texto2:\n",
    "            sim=lexsim(palabra1,palabra2)\n",
    "            if sim>maxsim:\n",
    "                maxsim=sim\n",
    "        suma+=maxsim\n",
    "    if len(texto1)==0:\n",
    "        return 0.0\n",
    "    return suma/len(texto1)\n",
    "\n",
    "\n",
    "def GEN_monge_elkan(texto1,texto2,lexsim, m=2):\n",
    "    suma=0.0\n",
    "    for palabra1 in texto1:\n",
    "        maxsim=0\n",
    "        for palabra2 in texto2:\n",
    "            sim=lexsim(palabra1,palabra2)\n",
    "            if sim>maxsim:\n",
    "                maxsim=sim\n",
    "        suma+=maxsim**m\n",
    "    if len(texto1)==0:\n",
    "        return 0.0\n",
    "    return (suma/len(texto1))**(1.0/m)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funcion para probar similitud de textos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "simiText = funcion de similitud textual\n",
    "similex = funcion de similitud Lexica\n",
    "m = parametro de funcion de similitud textual.\n",
    "'''\n",
    "def similitudTextos(simiText, simiLex, m=0): \n",
    "    suma_Pearson_r=0.0\n",
    "    suma_numero_de_pares=0\n",
    "    suma_Pearson_r_year={\"2012\":0.0,\"2013\":0.0,\"2014\":0.0,\"2015\":0.0,\"2016\":0.0}\n",
    "    suma_numero_de_pares_year={\"2012\":0,\"2013\":0,\"2014\":0,\"2015\":0,\"2016\":0}\n",
    "    \n",
    "    print \"%30s %30s %30s\" % (\"DATASET\",\"#pares\",\"Pearson r\")\n",
    "    for dataset in datasets:\n",
    "        predicciones=[]\n",
    "        d,gs=leer_dataset_textsim(dataset)\n",
    "        for texto1,texto2 in d:\n",
    "\n",
    "            # EJEMPLOS\n",
    "            #prediccion=STS_monge_elkan(texto1,texto2,lexsim.lex_sim_jaccard)\n",
    "            #prediccion=GEN_monge_elkan(texto1,texto2,lexsim.lex_sim_jaccard)\n",
    "            #prediccion=STS_monge_elkan(texto1,texto2,lexsim.lex_sim_Jaro)\n",
    "            #prediccion=STS_monge_elkan(texto1,texto2,lexsim.lex_sim_path_edit_distance) # ojo, demora muchísimo\n",
    "            #prediccion = STS_monge_elkan(texto1,texto2,lexsim.lex_sim_word2vec)\n",
    "            if m > 0:\n",
    "                prediccion = simiText(texto1,texto2,simiLex, m=m)\n",
    "            else:\n",
    "\n",
    "                prediccion = simiText(texto1,texto2,simiLex)\n",
    "            \n",
    "            predicciones.append(prediccion)\n",
    "        Pearson_r=pearsonr(gs,predicciones)[0]\n",
    "        print \"%30s %30s %30s\" % (dataset, str(len(d) ), str( round(Pearson_r,4) ))\n",
    "\n",
    "        #actualiza el promedio ponderado de todos los datasets\n",
    "        suma_Pearson_r+=Pearson_r*len(d)\n",
    "        suma_numero_de_pares+=len(d)\n",
    "        #actualiza el promedio ponderado por año\n",
    "        for year in suma_Pearson_r_year:\n",
    "            if year in dataset:\n",
    "                suma_Pearson_r_year[year]+=Pearson_r*len(d)\n",
    "                suma_numero_de_pares_year[year]+=len(d)\n",
    "         \n",
    "\n",
    "    print \"%30s %30s %30s\" % (\"Promedio ponderado todos\",suma_numero_de_pares,str(round(suma_Pearson_r/suma_numero_de_pares,4)))\n",
    "\n",
    "    mejor_en_SemEval={\n",
    "        \"2012\":\"0.6773 UKP\",\n",
    "        \"2013\":\"0.6181 UMBC\",\n",
    "        \"2014\":\"0.7610 DLS@CU\",\n",
    "        \"2015\":\"0.8015 DLS@CU\",\n",
    "        \"2016\":\"0.7781 Samsung Poland NLP\",\n",
    "        }\n",
    "    for year in [\"2012\",\"2013\",\"2014\",\"2015\",\"2016\"]:\n",
    "        print \"Promedio ponderado\",year,\"\\t\",suma_numero_de_pares_year[year],\"\\t\", \\\n",
    "              round(suma_Pearson_r_year[year]/suma_numero_de_pares_year[year],4),\"\\tMejor en SemEval:\\t\",mejor_en_SemEval[year]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "--- <function GEN_monge_elkan at 0x0D9204F0> , <function lex_sim_jaccard at 0x0D916AF0> \n",
      "\n",
      "                       DATASET                         #pares                      Pearson r\n",
      "         2012.input.MSRpar.txt                            750                         0.5063\n",
      "         2012.input.MSRvid.txt                            750                          0.334\n",
      "           2012.input.OnWN.txt                            750                         0.5856\n",
      "    2012.input.SMTeuroparl.txt                            459                         0.4876\n",
      "        2012.input.SMTnews.txt                            399                         0.5085\n",
      "           2013.input.FNWN.txt                            189                         0.2743\n",
      "      2013.input.headlines.txt                            750                         0.6343\n",
      "           2013.input.OnWN.txt                            561                         0.3408\n",
      "            2013.input.SMT.txt                            750                         0.3073\n",
      "     2014.input.deft-forum.txt                            450                           0.39\n",
      "      2014.input.deft-news.txt                            300                           0.61\n",
      "      2014.input.headlines.txt                            750                          0.589\n",
      "         2014.input.images.txt                            750                          0.534\n",
      "           2014.input.OnWN.txt                            750                         0.5201\n",
      "     2014.input.tweet-news.txt                            750                         0.7319\n",
      " 2015.input.answers-forums.txt                            375                         0.5171\n",
      "2015.input.answers-students.txt                            750                         0.6258\n",
      "         2015.input.belief.txt                            375                         0.6174\n",
      "      2015.input.headlines.txt                            750                         0.6333\n",
      "         2015.input.images.txt                            750                         0.6345\n",
      "  2016.input.answer-answer.txt                            254                         0.3515\n",
      "      2016.input.headlines.txt                            249                         0.6423\n",
      "     2016.input.plagiarism.txt                            230                         0.7453\n",
      "    2016.input.postediting.txt                            244                         0.7912\n",
      "2016.input.question-question.txt                            209                         0.1301\n",
      "      Promedio ponderado todos                          13294                          0.532\n",
      "Promedio ponderado 2012 \t3108 \t0.4814 \tMejor en SemEval:\t0.6773 UKP\n",
      "Promedio ponderado 2013 \t2250 \t0.4219 \tMejor en SemEval:\t0.6181 UMBC\n",
      "Promedio ponderado 2014 \t3750 \t0.5706 \tMejor en SemEval:\t0.7610 DLS@CU\n",
      "Promedio ponderado 2015 \t3000 \t0.6152 \tMejor en SemEval:\t0.8015 DLS@CU\n",
      "Promedio ponderado 2016 \t1186 \t0.5404 \tMejor en SemEval:\t0.7781 Samsung Poland NLP\n",
      "\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "--- <function GEN_monge_elkan at 0x0D9204F0> , <function lex_sim_jaccard_ngrams at 0x0D916B30> \n",
      "\n",
      "                       DATASET                         #pares                      Pearson r\n",
      "         2012.input.MSRpar.txt                            750                         0.5062\n",
      "         2012.input.MSRvid.txt                            750                         0.5372\n",
      "           2012.input.OnWN.txt                            750                         0.6026\n",
      "    2012.input.SMTeuroparl.txt                            459                         0.5218\n",
      "        2012.input.SMTnews.txt                            399                         0.4949\n",
      "           2013.input.FNWN.txt                            189                         0.3185\n",
      "      2013.input.headlines.txt                            750                         0.6222\n",
      "           2013.input.OnWN.txt                            561                         0.2714\n",
      "            2013.input.SMT.txt                            750                         0.3185\n",
      "     2014.input.deft-forum.txt                            450                          0.389\n",
      "      2014.input.deft-news.txt                            300                         0.5893\n",
      "      2014.input.headlines.txt                            750                         0.5914\n",
      "         2014.input.images.txt                            750                         0.5899\n",
      "           2014.input.OnWN.txt                            750                         0.4551\n",
      "     2014.input.tweet-news.txt                            750                         0.7567\n",
      " 2015.input.answers-forums.txt                            375                         0.5372\n",
      "2015.input.answers-students.txt                            750                         0.5439\n",
      "         2015.input.belief.txt                            375                         0.6302\n",
      "      2015.input.headlines.txt                            750                         0.6412\n",
      "         2015.input.images.txt                            750                         0.6679\n",
      "  2016.input.answer-answer.txt                            254                         0.2855\n",
      "      2016.input.headlines.txt                            249                         0.6256\n",
      "     2016.input.plagiarism.txt                            230                         0.7302\n",
      "    2016.input.postediting.txt                            244                         0.7834\n",
      "2016.input.question-question.txt                            209                         0.0799\n",
      "      Promedio ponderado todos                          13294                         0.5392\n",
      "Promedio ponderado 2012 \t3108 \t0.5378 \tMejor en SemEval:\t0.6773 UKP\n",
      "Promedio ponderado 2013 \t2250 \t0.408 \tMejor en SemEval:\t0.6181 UMBC\n",
      "Promedio ponderado 2014 \t3750 \t0.5724 \tMejor en SemEval:\t0.7610 DLS@CU\n",
      "Promedio ponderado 2015 \t3000 \t0.6092 \tMejor en SemEval:\t0.8015 DLS@CU\n",
      "Promedio ponderado 2016 \t1186 \t0.5093 \tMejor en SemEval:\t0.7781 Samsung Poland NLP\n",
      "\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "--- <function GEN_monge_elkan at 0x0D9204F0> , <function lex_sim_cosine at 0x0D916B70> \n",
      "\n",
      "                       DATASET                         #pares                      Pearson r\n",
      "         2012.input.MSRpar.txt                            750                         0.5133\n",
      "         2012.input.MSRvid.txt                            750                         0.2821\n",
      "           2012.input.OnWN.txt                            750                         0.5649\n",
      "    2012.input.SMTeuroparl.txt                            459                         0.4635\n",
      "        2012.input.SMTnews.txt                            399                         0.5241\n",
      "           2013.input.FNWN.txt                            189                         0.2649\n",
      "      2013.input.headlines.txt                            750                         0.6378\n",
      "           2013.input.OnWN.txt                            561                         0.3246\n",
      "            2013.input.SMT.txt                            750                         0.2856\n",
      "     2014.input.deft-forum.txt                            450                         0.3711\n",
      "      2014.input.deft-news.txt                            300                          0.617\n",
      "      2014.input.headlines.txt                            750                         0.5935\n",
      "         2014.input.images.txt                            750                         0.5204\n",
      "           2014.input.OnWN.txt                            750                         0.5043\n",
      "     2014.input.tweet-news.txt                            750                         0.7047\n",
      " 2015.input.answers-forums.txt                            375                         0.5068\n",
      "2015.input.answers-students.txt                            750                         0.6227\n",
      "         2015.input.belief.txt                            375                         0.5861\n",
      "      2015.input.headlines.txt                            750                         0.6337\n",
      "         2015.input.images.txt                            750                         0.6162\n",
      "  2016.input.answer-answer.txt                            254                         0.3324\n",
      "      2016.input.headlines.txt                            249                         0.6571\n",
      "     2016.input.plagiarism.txt                            230                         0.7353\n",
      "    2016.input.postediting.txt                            244                         0.7808\n",
      "2016.input.question-question.txt                            209                         0.1102\n",
      "      Promedio ponderado todos                          13294                         0.5195\n",
      "Promedio ponderado 2012 \t3108 \t0.464 \tMejor en SemEval:\t0.6773 UKP\n",
      "Promedio ponderado 2013 \t2250 \t0.411 \tMejor en SemEval:\t0.6181 UMBC\n",
      "Promedio ponderado 2014 \t3750 \t0.5585 \tMejor en SemEval:\t0.7610 DLS@CU\n",
      "Promedio ponderado 2015 \t3000 \t0.6047 \tMejor en SemEval:\t0.8015 DLS@CU\n",
      "Promedio ponderado 2016 \t1186 \t0.5318 \tMejor en SemEval:\t0.7781 Samsung Poland NLP\n",
      "\n",
      "---\n",
      "---\n",
      "---\n",
      "---\n",
      "--- <function GEN_monge_elkan at 0x0D9204F0> , <function lex_sim_Jaro at 0x0D916BB0> \n",
      "\n",
      "                       DATASET                         #pares                      Pearson r\n",
      "         2012.input.MSRpar.txt                            750                          0.539\n",
      "         2012.input.MSRvid.txt                            750                         0.3385\n",
      "           2012.input.OnWN.txt                            750                         0.5882\n",
      "    2012.input.SMTeuroparl.txt                            459                         0.5025\n",
      "        2012.input.SMTnews.txt                            399                         0.5059\n",
      "           2013.input.FNWN.txt                            189                         0.2675\n",
      "      2013.input.headlines.txt                            750                          0.658\n",
      "           2013.input.OnWN.txt                            561                         0.3334\n",
      "            2013.input.SMT.txt                            750                         0.2707\n",
      "     2014.input.deft-forum.txt                            450                         0.3763\n",
      "      2014.input.deft-news.txt                            300                          0.618\n",
      "      2014.input.headlines.txt                            750                         0.6176\n",
      "         2014.input.images.txt                            750                         0.5171\n",
      "           2014.input.OnWN.txt                            750                         0.5109\n",
      "     2014.input.tweet-news.txt                            750                          0.699\n",
      " 2015.input.answers-forums.txt                            375                         0.5131\n",
      "2015.input.answers-students.txt                            750                         0.6261\n",
      "         2015.input.belief.txt                            375                         0.5607\n",
      "      2015.input.headlines.txt                            750                         0.6641\n",
      "         2015.input.images.txt                            750                         0.6286\n",
      "  2016.input.answer-answer.txt                            254                          0.319\n",
      "      2016.input.headlines.txt                            249                         0.6824\n",
      "     2016.input.plagiarism.txt                            230                         0.7366\n",
      "    2016.input.postediting.txt                            244                         0.7879\n",
      "2016.input.question-question.txt                            209                         0.1416"
     ]
    }
   ],
   "source": [
    "# ESTE \"IF\" ES PARA QUE LA SIGUIENTE PARTE DEL CODIGO NO SE EJECUTE CUANDO ESTE PROGRAMA SE IMPORTE EN OTRO PROGRAMA CON import textsim\n",
    "if __name__ == '__main__':  \n",
    "    funciones = [GEN_monge_elkan, STS_monge_elkan]\n",
    "    '''\n",
    "    vec_sim_lex = [lexsim.lex_sim_jaccard, lexsim.lex_sim_jaccard_ngrams, lexsim.lex_sim_cosine, lexsim.lex_sim_Jaro, \\\n",
    "                    lexsim.lex_sim_edit_distance, lexsim.lex_sim_path, lexsim.lex_sim_lch, lexsim.lex_sim_wup, \\\n",
    "                    lexsim.lex_sim_res, lexsim.lex_sim_jcn, lexsim.lex_sim_lin, lexsim.lex_sim_word2vec, \\\n",
    "                    lexsim.lex_sim_path_edit_distance, lexsim.lex_sim_path_jaccard_23grams_porter]\n",
    "    '''\n",
    "    vec_sim_lex = [lexsim.lex_sim_jaccard, lexsim.lex_sim_jaccard_ngrams, lexsim.lex_sim_cosine, lexsim.lex_sim_Jaro, \\\n",
    "                lexsim.lex_sim_path, lexsim.lex_sim_lch, lexsim.lex_sim_wup, lexsim.lex_sim_res, lexsim.lex_sim_jcn,  \\\n",
    "                lexsim.lex_sim_lin, lexsim.lex_sim_word2vec, lexsim.lex_sim_path_jaccard_23grams_porter]\n",
    "    #vec_sim_lex = [lexsim.lex_sim_jaccard]                \n",
    "    for fun in funciones:\n",
    "        for s in vec_sim_lex:\n",
    "            print \"\\n---\"*5, fun,\", "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clase 21/06/2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
